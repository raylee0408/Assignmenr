# Import the Quix Streams modules for interacting with Kafka:
from quixstreams.sources import BaseSource
from quixstreams.models.messages import KafkaMessage

import pandas as pd
import json
import random
import time

class CsvSource(BaseSource):
    
    def __init__(self, filename: str, sleep_between_rows: float = 0) -> None:
        self.filename = filename
        self.sleep_between_rows = sleep_between_rows
        
        self.message_key = f"{self.filename}_{str(random.randint(1, 100)).zfill(3)}"
        
        df = pd.read_csv(filename)
        print("File loaded.")
        
        # Get the column headers as a list
        self.headers = df.columns.tolist()
        
        self._iterator = df.iterrows()
            
        
        super().__init__()
        
    
    def poll(self):
        
        _, row = next(self._iterator)
        # Create a dictionary that includes both column headers and row values
        # Replace NaN values with None (which will become 'null' in JSON)
        row_data = {header: (None if pd.isna(row[header]) else row[header]) for header in self.headers}

        # We are simulating data so removing original timestamp.
        # We will use Kafka autogenerated timestamp instead.
        del row_data["Timestamp"]
        
        # Serialize row_data to a JSON string
        json_data = json.dumps(row_data)

        time.sleep(self.sleep_between_rows)
        
        # publish the data to the topic
        return KafkaMessage(
            key=self.message_key,
            value=json_data,
            headers={}
        )